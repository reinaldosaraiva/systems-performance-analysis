# üöÄ PR√ìXIMOS PASSOS - Otimiza√ß√£o Multi-Agente

## üìÖ Para Amanh√£

---

## üéØ OBJETIVO PRINCIPAL: Reduzir Lat√™ncia de 70s para ~15s

### Problema Atual:
- **Tempo atual:** 70 segundos (6 chamadas sequenciais)
- **Gargalo:** Agentes executam sequencialmente (um por vez)
- **Solu√ß√£o:** Usar `asyncio.gather()` para execu√ß√£o paralela

### Benef√≠cio Esperado:
- **Tempo esperado:** 10-15 segundos (6 chamadas paralelas)
- **Ganho:** 5x mais r√°pido!

---

## üìã TAREFA 1: Paralelizar Chamadas dos Agentes

### Arquivo: `src/infrastructure/ai/autogen_multiagent.py`

### Mudan√ßa:

#### Antes (Sequencial - 70s):
```python
for agent in self.agents:
    logger.info(f"Calling agent: {agent['name']}")
    response = await self._call_ollama(prompt)
    agent_analyses.append({...})
```

#### Depois (Paralelo - 15s):
```python
import asyncio

# Criar tasks para todos os agentes
async def call_agent(agent):
    prompt = f"""{agent['system_message']}

Scenario: {context}

Analyze from your {agent['role']} perspective."""

    try:
        response = await self._call_ollama(prompt)
        return {
            "agent": agent["name"],
            "role": agent["role"],
            "analysis": response
        }
    except Exception as e:
        logger.error(f"Error calling agent {agent['name']}: {e}")
        return None

# Executar todos em paralelo
tasks = [call_agent(agent) for agent in self.agents]
results = await asyncio.gather(*tasks, return_exceptions=True)

# Filtrar resultados v√°lidos
agent_analyses = [r for r in results if r is not None and not isinstance(r, Exception)]
```

### Passos:
1. ‚úÖ Importar `asyncio`
2. ‚úÖ Criar fun√ß√£o `call_agent()` async
3. ‚úÖ Usar `asyncio.gather()` para executar tudo junto
4. ‚úÖ Filtrar exce√ß√µes com `return_exceptions=True`
5. ‚úÖ Testar e validar tempo

---

## üìã TAREFA 2: Adicionar Timeout por Agente

### Problema:
Se um agente travar, n√£o deve bloquear os outros

### Solu√ß√£o:
```python
import asyncio

async def call_agent_with_timeout(agent, timeout=30):
    """Call agent with timeout protection."""
    try:
        return await asyncio.wait_for(
            call_agent(agent),
            timeout=timeout
        )
    except asyncio.TimeoutError:
        logger.warning(f"Agent {agent['name']} timed out after {timeout}s")
        return None

# Uso:
tasks = [call_agent_with_timeout(agent, timeout=30) for agent in self.agents]
results = await asyncio.gather(*tasks, return_exceptions=True)
```

### Benef√≠cio:
- Agentes lentos n√£o bloqueiam os r√°pidos
- Sistema mais resiliente
- Melhor experi√™ncia do usu√°rio

---

## üìã TAREFA 3: Otimizar Consolida√ß√£o

### Op√ß√£o A: Consolidar enquanto agentes rodam
```python
# Assim que 3+ agentes responderem, come√ßar consolida√ß√£o
# N√£o esperar todos os 5

async def smart_consolidation(agent_analyses, min_agents=3):
    """Start consolidation as soon as we have minimum agents."""
    if len(agent_analyses) >= min_agents:
        return await self._consolidate_analyses(agent_analyses, context)
    else:
        return self._create_fallback_consolidated_insight(agent_analyses)
```

### Op√ß√£o B: Consolida√ß√£o paralela
```python
# Enquanto √∫ltimo agente responde, j√° processar os anteriores
consolidation_task = asyncio.create_task(
    self._consolidate_analyses(agent_analyses[:4], context)
)
last_agent = await call_agent(self.agents[4])
consolidated = await consolidation_task
```

---

## üìã TAREFA 4: Caching Inteligente

### Implementar Cache Redis/Mem√≥ria

```python
import hashlib
from typing import Optional

class CachedAutoGenMultiAgent(AutoGenMultiAgent):
    def __init__(self, *args, cache_ttl=300, **kwargs):
        super().__init__(*args, **kwargs)
        self.cache = {}  # Ou Redis
        self.cache_ttl = cache_ttl  # 5 minutos

    def _generate_cache_key(self, context: str) -> str:
        """Generate cache key from context."""
        return hashlib.md5(context.encode()).hexdigest()

    async def analyze_collaborative(
        self,
        metrics: Optional[SystemMetrics] = None,
        max_rounds: int = 2,
        use_cache: bool = True
    ) -> Dict[str, Any]:
        """Analyze with caching support."""

        context = self._build_analysis_context(metrics)
        cache_key = self._generate_cache_key(context)

        # Check cache
        if use_cache and cache_key in self.cache:
            cached = self.cache[cache_key]
            if time.time() - cached['timestamp'] < self.cache_ttl:
                logger.info("Returning cached analysis")
                return cached['result']

        # Normal analysis
        result = await super().analyze_collaborative(metrics, max_rounds)

        # Store in cache
        self.cache[cache_key] = {
            'timestamp': time.time(),
            'result': result
        }

        return result
```

### Benef√≠cio:
- An√°lises id√™nticas retornam instantaneamente
- Reduz custos de chamadas LLM
- Melhor para dashboards com refresh frequente

---

## üìã TAREFA 5: M√©tricas e Observabilidade

### Adicionar m√©tricas de performance:

```python
import time
from dataclasses import dataclass

@dataclass
class AgentMetrics:
    agent_name: str
    duration: float
    response_size: int
    success: bool
    error: Optional[str] = None

class InstrumentedAutoGenMultiAgent(AutoGenMultiAgent):
    async def call_agent_with_metrics(self, agent) -> tuple[dict, AgentMetrics]:
        """Call agent and collect metrics."""
        start = time.time()

        try:
            result = await call_agent(agent)
            duration = time.time() - start

            metrics = AgentMetrics(
                agent_name=agent['name'],
                duration=duration,
                response_size=len(result['analysis']),
                success=True
            )

            return result, metrics
        except Exception as e:
            duration = time.time() - start
            metrics = AgentMetrics(
                agent_name=agent['name'],
                duration=duration,
                response_size=0,
                success=False,
                error=str(e)
            )
            return None, metrics

    async def analyze_collaborative(self, ...):
        """Enhanced analysis with metrics."""
        tasks = [self.call_agent_with_metrics(agent) for agent in self.agents]
        results = await asyncio.gather(*tasks)

        # Separate results and metrics
        analyses = [r[0] for r in results if r[0] is not None]
        metrics = [r[1] for r in results]

        # Log metrics
        for m in metrics:
            logger.info(f"Agent {m.agent_name}: {m.duration:.2f}s, "
                       f"size={m.response_size}, success={m.success}")

        # Continue with consolidation...
```

---

## üìã TAREFA 6: Estrat√©gia de Fallback Progressivo

### N√≠veis de qualidade baseados em quantos agentes responderam:

```python
async def adaptive_consolidation(agent_analyses, context):
    """Adapt quality based on number of agents."""

    num_agents = len(agent_analyses)

    if num_agents >= 5:
        # Todos responderam - an√°lise completa
        confidence = 95
        quality = "excellent"
    elif num_agents >= 3:
        # Maioria respondeu - an√°lise boa
        confidence = 88
        quality = "good"
    elif num_agents >= 2:
        # M√≠nimo respondeu - an√°lise b√°sica
        confidence = 80
        quality = "basic"
    else:
        # S√≥ 1 ou nenhum - fallback
        return self._create_fallback_consolidated_insight(agent_analyses)

    # Consolidar com qualidade adaptativa
    insights = await self._consolidate_analyses(agent_analyses, context)

    # Ajustar confidence baseado em quantos agentes participaram
    for insight in insights:
        insight['confidence'] = confidence
        insight['quality'] = quality
        insight['agents_participated'] = num_agents

    return insights
```

---

## üìä COMPARA√á√ÉO ESPERADA

### Antes (Atual):
```
Single-Agent:    ~7 segundos  (85% confidence)
Multi-Agent:    ~70 segundos  (92% confidence)
```

### Depois (Otimizado):
```
Single-Agent:    ~7 segundos  (85% confidence)
Multi-Agent:    ~15 segundos  (92% confidence)  ‚Üê 5x mais r√°pido!
Multi-Agent (cached): <1 segundo  (92% confidence)
```

---

## üéØ ORDEM DE IMPLEMENTA√á√ÉO RECOMENDADA

### Dia 1 (Amanh√£):
1. ‚úÖ **TAREFA 1**: Paralelizar agentes com `asyncio.gather()` (PRIORIDADE ALTA)
   - Impacto: 70s ‚Üí 15s (5x mais r√°pido)
   - Complexidade: M√©dia
   - Tempo estimado: 1-2 horas

2. ‚úÖ **TAREFA 2**: Adicionar timeout por agente
   - Impacto: Resili√™ncia
   - Complexidade: Baixa
   - Tempo estimado: 30 minutos

3. ‚úÖ **TAREFA 5**: M√©tricas b√°sicas
   - Impacto: Observabilidade
   - Complexidade: Baixa
   - Tempo estimado: 30 minutos

### Dia 2 (Depois):
4. ‚úÖ **TAREFA 4**: Caching (Redis ou mem√≥ria)
   - Impacto: 15s ‚Üí <1s para an√°lises repetidas
   - Complexidade: M√©dia
   - Tempo estimado: 2-3 horas

5. ‚úÖ **TAREFA 3**: Consolida√ß√£o inteligente
   - Impacto: -2s adicional
   - Complexidade: M√©dia
   - Tempo estimado: 1 hora

6. ‚úÖ **TAREFA 6**: Fallback progressivo
   - Impacto: Melhor UX
   - Complexidade: Baixa
   - Tempo estimado: 1 hora

---

## üìù C√ìDIGO DE EXEMPLO COMPLETO (TAREFA 1)

### Arquivo: `src/infrastructure/ai/autogen_multiagent.py`

```python
async def analyze_collaborative(
    self,
    metrics: Optional[SystemMetrics] = None,
    max_rounds: int = 2
) -> Dict[str, Any]:
    """Run collaborative analysis with all agents (PARALLEL)."""

    logger.info(f"Starting PARALLEL analysis with {len(self.agents)} agents")

    try:
        # Build analysis context
        context = self._build_analysis_context(metrics)

        # Define async function to call each agent
        async def call_single_agent(agent):
            """Call a single agent with error handling."""
            logger.info(f"Calling agent: {agent['name']}")

            try:
                # Create agent-specific prompt
                prompt = f"""{agent['system_message']}

Scenario: {context}

Analyze from your {agent['role']} perspective and provide 2-3 actionable recommendations.
Respond with JSON only."""

                # Call Ollama
                response = await self._call_ollama(prompt)

                logger.info(f"Agent {agent['name']} completed ({len(response)} chars)")

                return {
                    "agent": agent["name"],
                    "role": agent["role"],
                    "analysis": response
                }

            except Exception as e:
                logger.error(f"Error calling agent {agent['name']}: {e}")
                return None

        # Execute all agents in PARALLEL using asyncio.gather()
        logger.info("Executing all agents in PARALLEL...")
        start_time = time.time()

        tasks = [call_single_agent(agent) for agent in self.agents]
        results = await asyncio.gather(*tasks, return_exceptions=True)

        duration = time.time() - start_time
        logger.info(f"All agents completed in {duration:.2f}s")

        # Filter valid results
        agent_analyses = []
        for result in results:
            if result is not None and not isinstance(result, Exception):
                agent_analyses.append(result)
            elif isinstance(result, Exception):
                logger.error(f"Agent failed with exception: {result}")

        logger.info(f"Successfully collected {len(agent_analyses)}/{len(self.agents)} agent analyses")

        # Consolidate all analyses into final insights
        insights = await self._consolidate_analyses(agent_analyses, context)

        logger.info(f"Collaborative analysis completed with {len(insights)} insights")

        return {
            "status": "success",
            "agents_participated": len(agent_analyses),
            "execution_time": f"{duration:.2f}s",
            "rounds": 1,
            "insights": insights,
            "collaboration_summary": f"Parallel analysis from {len(agent_analyses)} specialized agents"
        }

    except Exception as e:
        logger.error(f"Error in collaborative analysis: {e}")
        return {
            "status": "error",
            "error": str(e),
            "insights": []
        }
```

---

## ‚úÖ CHECKLIST PARA AMANH√É

### Setup:
- [ ] Abrir projeto no editor
- [ ] Ativar ambiente virtual: `source .venv/bin/activate`
- [ ] Verificar Ollama rodando: `curl http://localhost:11434/api/version`

### Implementa√ß√£o:
- [ ] TAREFA 1: Paralelizar agentes com `asyncio.gather()`
- [ ] Adicionar import `import asyncio` no topo
- [ ] Criar fun√ß√£o `call_single_agent()` async
- [ ] Substituir loop `for` por `asyncio.gather()`
- [ ] Testar localmente (deve reduzir de 70s para ~15s)

### Testes:
- [ ] Rebuild Docker: `docker compose build brendan-api`
- [ ] Restart: `docker compose up -d brendan-api`
- [ ] Testar endpoint: `time curl -s http://localhost:8080/api/insights/autogen`
- [ ] Verificar tempo (esperado: 10-20s)
- [ ] Verificar logs: `docker compose logs brendan-api | grep "completed in"`

### Valida√ß√£o:
- [ ] Comparar tempo antes (70s) vs depois (15s)
- [ ] Verificar qualidade dos insights (deve manter 92% confidence)
- [ ] Confirmar que todos os 5 agentes est√£o participando
- [ ] Testar m√∫ltiplas chamadas para validar consist√™ncia

### Commit:
- [ ] `git add src/infrastructure/ai/autogen_multiagent.py`
- [ ] `git commit -m "perf: parallelize multi-agent calls with asyncio (70s‚Üí15s)"`
- [ ] `git log --oneline -1` (verificar)

---

## üéØ RESULTADO ESPERADO

**Antes:**
```bash
$ time curl -s http://localhost:8080/api/insights/autogen
real    1m10.76s  # 70 segundos
```

**Depois:**
```bash
$ time curl -s http://localhost:8080/api/insights/autogen
real    0m15.23s  # 15 segundos (5x mais r√°pido!)
```

---

## üìö REFER√äNCIAS

### Asyncio Documentation:
- https://docs.python.org/3/library/asyncio-task.html
- `asyncio.gather()` - Run tasks concurrently
- `asyncio.wait_for()` - Add timeout to coroutine
- `asyncio.create_task()` - Schedule coroutine

### C√≥digo Similar:
- `src/infrastructure/ai/ollama_llm_client.py` - J√° usa httpx async
- `src/application/use_cases/performance/get_autogen_insights.py` - Use case async

---

## üí° DICAS

1. **Teste localmente primeiro** antes do Docker
2. **Use logs** para debugar timing: `logger.info(f"Agent {name} took {duration}s")`
3. **Comece simples** - asyncio.gather() b√°sico antes de otimiza√ß√µes avan√ßadas
4. **Valide a qualidade** - garantir que paraleliza√ß√£o n√£o afeta resultado
5. **Monitore mem√≥ria** - 6 chamadas simult√¢neas ao Ollama consomem mais RAM

---

## üöÄ BONUS: Otimiza√ß√£o Extrema (Futuro)

Se quiser ir al√©m de 15s ‚Üí 10s:

### Streaming Responses:
```python
# Em vez de aguardar resposta completa, processar streaming
async for chunk in ollama_stream(prompt):
    partial_analysis += chunk
    # Come√ßar consolida√ß√£o com an√°lises parciais
```

### Modelos Menores para Agentes:
```python
# Usar modelo menor/mais r√°pido para agentes individuais
agent_model = "mistral:7b"  # Mais r√°pido
consolidator_model = "minimax-m2:cloud"  # Mais poderoso
```

### Connection Pooling:
```python
# Reutilizar conex√µes HTTP
self.client = httpx.AsyncClient(
    timeout=120,
    limits=httpx.Limits(max_connections=10)
)
```

---

**Boa sorte amanh√£! üöÄ**

**Meta:** Reduzir de 70s para 15s com `asyncio.gather()` ‚ú®
